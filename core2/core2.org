* Core2
** <2021-01-19 Tue>

*** Getting used to Arrow

tx-ingest index
- row index

#+begin_src clojure
{:row-ids [0 1 2 3]
 :blobs [{:battery-level 0.84
          :ssid :s1}
         {:battery-level 0.12
          :ssid :s2}
         ...]}
#+end_src

per tx
- column of pairs: row-id, nippy blob
- column of pairs: row-id, arrow IPC message

=VariableLengthList<Union<...>>=

content index

if this were 'normal, schema-aot relations'
schema:
'each record block has row-ids (long), ssid (string), battery-level (double)'

record batch:
metadata (count of records) + extension (minmax, bloom filter, unique count, histogram, etc, per column)
could store metadata inline (using Arrow's built-in metadata support) or separately (different file)
one length-3 record batch would be =0 2 5, :s1 :s2 :s4, 0.84 0.12 0.56=

metadata for entire file - end of file or own file

block - range of bytes
'schema block' - how to interpret record batch blocks
'record batch blocks' - columnar data
  not self-describing without schema block
'dictionary block' - mapping from ids to strings
'dictionary delta block' - changes from the previous dictionary

ingest for a batch of transactions

**** content-idx options

***** option A - all columns in one file
#+begin_src clojure
  {:row-ids [0 1 2 5]
   :crux.db/id [.. ..]
   :ssids [:s1 nil :s2 :s4]
   :battery-level [0.84 nil 0.12 0.56]
   :user-name [nil "James" nil nil]}

  ;; + 'real' Arrow file
  ;; + row-id stored once (not massive, but still a benefit)
  ;; - loads of nils for dynamic data?
#+end_src

***** option B - file per column

#+begin_src clojure
;; chunk 1, file 0
{:row-ids [0 2 5]
 :ssids [:s1 :s2 :s4]}

;; chunk 1, file 1
{:row-ids [0 2 5]
 :battery-level [0.84 0.12 0.56]}

;; chunk 1, file 2
{:row-ids [1]
 :user-name ["James"]}
#+end_src

projection is hard here
can we align row-ids between files in a chunk? yes, possibly, using the validity bitmap
can we align blocks between the files in a chunk?

***** option B2 - alignment

#+begin_src clojure
;; chunk 1, file 0
{:ssids [:s1 nil :s2 nil nil :s4]}

;; chunk 1, file 1
{:battery-level [0.84 nil 0.12 nil nil 0.56]}

;; chunk 1, file 2
{:user-name [nil "James" nil nil nil]} ; or truncated
#+end_src

***** option C - hybrid - file per schema
tx-ingest index
- a: don't have it
- b: use it for projections

#+begin_src clojure

  ;; row per operation
  [{:op :put
    :row-id 41
    :doc {} ; union of structs? byte array? arrow extension type - variable length byte array?
    :start-valid-time #inst "2021"
    :tx-id 15
    :tx-time #inst "2021"
    }

   {:op :delete
    :row-id 41}]

  ;; hybrid - no schema re-use
  [{:tx-time ...
    :tx-id ...
    :ops [{:op :put
           :row-id 41
           :schema-id 12
           :doc 'ipc-message}
          {:op :put
           :row-id 42
           :schema-id 12
           :doc 'ipc-message}]}]

  ;; hybrid - with schema re-use
  [{:tx-time ...
    :tx-id ...
    :ops [{:op :put
           :row-id 41
           :schema-id 12
           :doc 'record-batch-block}
          {:op :put
           :row-id 42
           :schema-id 12
           :doc 'record-batch-block}]
    :union-schema 'schema}]

  ;; row per transaction
  [{:tx-time ...
    :tx-id ...
    :ops 'ipc-message}]

  ;; should this contain transaction metadata too?
  ;; block per transaction? - WAL

  ;; what's the format on Kafka? Arrow? IPC?
  ;; would need less of a serde before ingest
  ;; parallelism - removes work from the critical serialised section
  ;; messages:
  ;; - schema: union of all docs in the transaction
  ;; - 1 record-batch
  ;; - row per operation

  ;; no longer infinite retention
  ;; no longer evicting from the tx-log

  ;; could steal some ideas from Flight
  ;; Kafka - schema registry - no longer self-describing though

  ;; alternative: row per transaction

  ;;;; content index

  [{:row-ids [0 2 5]
    :crux.db/id [... ... ..]
    :ssids [:s1 nil :s4]
    :battery-level [0.84 0.12 0.56]}

   {:row-ids [1]
    :user-name ["James"]}]

  [{:live-file 0
    :cols #{:crux.db/id :ssids :battery-level}}
   {:live-file 1
    :cols #{:crux.db/id :user-name}}
   {:live-file 2
    :cols #{:crux.db/id :ssids}}]
#+end_src

in option C, how do we make the cutoff?
same problem as option B, I think?
maybe when one of the live files goes over an AV limit
(using AVs as the limit as this is the metric that file size is most proportional to?)
could use transactions, or rows, but AVs is more granular - maybe bytes?
could use something more complex that takes all the live files into account - so long as it's deterministic, it doesn't really matter

might have to pull down many chunks to find `:name "James"` - particularly `:crux.db/id` (used within joins)

need explicit row ids because documents have dynamic structure
should we optimise for cold nodes answering low-latency queries? hard...

**** questions

cardinality many? vectors/sets
1. ignore
2. solve, but just vectors (variable length lists) - minmax/bloom would flatten
2b. add an extension type for sets
3. duplicated row-ids (works for B)

entity id as a column?
works well in options A/C - entity ID is right next door to the data

maps as values - nippy? nested struct?

metadata 'latest-completed-tx'
metadata 'latest row-id'

**** unions
dense:: Pair<List<Double>, List<Long>> + Map<Idx, Type & Offset>
^^ we probably want this one

sparse:: List<Pair<Double, Long>>
#+begin_src clojure
[[0.0 nil]
 [nil 4]
 [12.0 nil]]
#+end_src

*** access patterns
AVE AV AEV AE, project-*

#+begin_src clojure
  '{:find [?a],
    :where [[?e :name "Ivan"],
            [?e :age ?a]]}
#+end_src

find =:name= "Ivan", get a bunch of row-ids, look up =:age= for those row-ids

#+begin_src clojure
  '{:find [?n], :where [[_ :name ?n]]}
#+end_src
scan =:name= cols
currently: scan AE -> filter from bitemp
B is preferable here - general 'projecting a small subset' advantage
parameter: visibility ratio
bitemp index split by attribute (/set of attributes) -> scan for row-ids,

project-* works quite nicely in C and A - the whole document is in the same record batch
downside is that projecting a single field requires transferring the whole doc.

'pull-like', navigational join
#+begin_src clojure
  '{:find [?order #_?line-item ?product-name]
    :where [[?order :order-id "1234"]
            [?line-item :order ?order]
            [?line-item :product ?product]
            [?product :product-name ?product-name]]}

  '{:find [(pull ?order [{:line-item {:product [:product-name]}}])]
    :where [[?order :order-id "1234"]]}
#+end_src

'proper' join
#+begin_src clojure
'{:find [?student]
  :where [[?enrollment1 :student ?student]
          [?enrollment1 :course :maths]
          [?enrollment2 :student ?student]
          [?enrollment2 :course :science]]}
#+end_src clojure

assume science is smaller

intersection
nested-loop join - if both sides are really small
hash join - default to this
  hash of student to enrollment for the smaller, scan larger
  can spill hash buckets to disk if need be
sort/merge join

*** temporal use cases
temporal parameters
number of visible entities?
how many updates to any one entity?

assumption: entity/vt pair, 0-5 tt updates
would you _model_ any cases this way?

tweets
ever-growing, ~all visible now, edit/delete in vt=now 0-5 times, no tt updates
possible evict
delete cascade

customer details
relatively constant cardinality, ~all visible now, tens of vt updates/deletes, 0-5 tt updates for a vt

orders
how to model 'status'? either delete when complete, or set status flag (JH preferred)
ever growing cardinality, ~all visible now, tens of vt updates/deletes, 0-5 tt updates for a vt

active sessions - transient
ever growing cardinality, few visible now, delete when you're done

share prices - time series
model as vt being the current share price
relatively constant cardinality, ~all visible now, small visible set, loads of vt updates/deletes, 0-5 tt updates for a vt
or, model as events -> ever growing cardinality, few vt updates, few tt updates

common denominator - entity identity
order-id is the identity - valid time represents that order changing over time
share ticker is the identity - likewise

*** implementation ideas
no natural clustering by ticker
cluster by row-id
batch would then have 16 files - tradeoff is searching by anything you _haven't_ clustered by is painful

what if we could assume that vt=tt for the 'majority of updates'?
bitemp index aims: immutable, append-only, columnar

roughly sorting by VT is still append-only (except when it's not)
sorting by TT _is_ append-only - but we don't often want to sort first by TT
sort by VT within a bitemp index chunk

in bitemp index, need to find based on row-id
or remove row-id? VT as a column

let's say we have hundreds of bitemp index chunks
- 'is this row-id valid at the time of this query?' (timeslice)
- 'what is the current row-id for this entity-id?' - include the (hash of the) entity-id in the bitemp index, as a column?
- 'when was this row-id valid?' (temporal ranges)

attribute (sets) slicing would be worth exploring

[?e :name "Ivan"]
#{:name :age} -> bitemp index slice

modelling VT corrections like evictions?

ok. let's say bitemp index had entity-id (hash), row-id (+ve/-ve), vt, tt cols
overall file would have metadata
- attribute sets contained
- entity id, abs(row-id) bloom filters (on record batches too)
- vt histogram/minmax, tt minmax (likewise)

ingest would need to make queries to generate list of row-ids affected by tx-op (same as currently)

questions from above:
- 'is this row-id valid at the time of this query?' (timeslice)
  - chances are we'd have attribute set in hand (because we've come from the content idx)
  - only need to check bitemp index files with vt/tt less than query time
  - could say 'definitely no/maybe yes' from bloom filters
- 'what is the current row-id for this entity-id?'
  - entity-id bloom filter, starting at file with latest vts/tts
- 'when was this row-id valid?' (temporal ranges)
  - just check row-id bloom filters, ignore above temporal filters

** <2021-01-25 Mon>
*** Metadata
Problem: how do we efficiently intersect content + bitemporal?
Rocks had compaction - there'd only be one entry for one AVE - we have them spread over many chunks
We don't have an easy way of knowing whether the entry in one chunk is the same entity as we've already seen, albeit with a different row id

idea: metadata - current vs historical?
- would mean revisions to metadata
- revisions would be heavy, they'd need to scan the whole block.
